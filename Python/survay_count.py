# -*- coding: utf-8 -*-
"""
Created on Sat Jan 23 12:07:25 2021

@author: andy
"""

# Generated by Selenium IDE
import pytest
import time
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support import expected_conditions
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
from bs4 import BeautifulSoup
import pandas as pd
import os
import glob
import shutil
import pandas as pd
from pathlib import Path
import re 
import matplotlib.pyplot as plt
import numpy as np
import math
from datetime import datetime

from os import path
from docx import Document
from sklearn.linear_model import LinearRegression

#LDA Imports
import gensim
import nltk
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
from gensim import corpora, models
from pprint import pprint
np.random.seed(2018)
#nltk.download('wordnet')

#plot
import matplotlib.pyplot as plt
import seaborn as sns

#Lexicon sentiment analysis 
from afinn import Afinn
from textblob import TextBlob


from sklearn.metrics import r2_score

if __name__ == '__main__':
    # =============================================================================
    # Global varaibles
    # =============================================================================
    question_statistics=pd.DataFrame()
    difficult_threshold={"min":0.3,"max":0.9}
    stemmer = SnowballStemmer('english')
    
    # =============================================================================
    # Functions
    # =============================================================================
    def get_file_path(directory_name,filename):    
        director_path=Path(directory_name)
        full_file_path=director_path / filename
        return full_file_path
        
    def lemmatise_stemming(word):
        return stemmer.stem(WordNetLemmatizer().lemmatize(word, pos='v'))
    
    def preprocess(text):
        result = []
        for token in gensim.utils.simple_preprocess(text):
            if token not in gensim.parsing.preprocessing.STOPWORDS:
                result.append(lemmatise_stemming(token))
        return result
    
    # # =============================================================================
    # # Import Data  
    # # =============================================================================
    
    
    apprentice_EPA_feedback_path=get_file_path(r"C:\Users\Andy.JIVEDIVE.000\OneDrive - University of Buckingham\Assignments\Final Project\Archive","Apprentice End-point Assessment (EPA) Feedback 1.xlsx")
    
    
    #dataset=pd.read_excel(data_orginal_file, sheet_name="TestData",skiprows=range(1,1))
 
    #Read in comments
    apprentice_EPA_feedback_df=pd.read_excel(apprentice_EPA_feedback_path)
    apprentice_EPA_comments_df=(apprentice_EPA_feedback_df.iloc[0:-1,20])
 
    #Read in overall scores
    index_of_null_comments=np.where(apprentice_EPA_feedback_df.iloc[0:-1,20].notnull())[0]
    apprentice_EPA_overallscore_df=apprentice_EPA_feedback_df.loc[index_of_null_comments]
    apprentice_EPA_overallscore_df=apprentice_EPA_overallscore_df.iloc[0:-1,19]
    apprentice_EPA_comments_df=apprentice_EPA_comments_df.dropna()
    wordlen=[]
    
    for row in apprentice_EPA_comments_df:
        #len=len(row.split)
        l=len(row.split())
        wordlen.append(l)
        print(l)
        
    ax = sns.boxplot(x=wordlen)
    # Title & Subtitle
    plt.suptitle('Distribution of the number of words in survey responses')

    # write the Sigmoid formula
    plt.xlabel(r'Number of words')
    plt.show()